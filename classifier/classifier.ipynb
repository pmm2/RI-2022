{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de links de treino: 58\n",
      "Quantidade de links de teste: 12\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data_file = open(\"./training_data_set.json\")\n",
    "raw_data = json.load(data_file)\n",
    "data_file.close()\n",
    "\n",
    "training_sites = raw_data.get(\"trainingSitesAndTags\", [])\n",
    "print(\"Quantidade de links de treino:\", len(training_sites))\n",
    "\n",
    "data_file = open(\"./testing_data_set.json\")\n",
    "raw_data = json.load(data_file)\n",
    "data_file.close()\n",
    "\n",
    "testing_sites = raw_data.get(\"testingSitesAndTags\", [])\n",
    "print(\"Quantidade de links de teste:\", len(testing_sites))\n",
    "\n",
    "testing_url_list = []\n",
    "testing_tags_list = []\n",
    "for [url, tag] in testing_sites:\n",
    "    testing_url_list.append(url)\n",
    "    testing_tags_list.append(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treating raw data for training models.\n",
    "First, we need to actually retrieve the content present in the url.\n",
    "Then, we strip the content of its HTML and style tags, as they are irrelevant to our classification. For this, we use BeautifulSoup decompose function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to remove tags\n",
    "def remove_tags(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "  \n",
    "    for data in soup(['style', 'script']):\n",
    "        data.decompose()\n",
    "  \n",
    "    return ' '.join(soup.stripped_strings)\n",
    "\n",
    "sites_contents = []\n",
    "y_sites = []\n",
    "\n",
    "for [url, usefulness] in training_sites:\n",
    "    r = requests.get(url)\n",
    "    filtered_content = remove_tags(r.content)\n",
    "    sites_contents.append(filtered_content)\n",
    "    y_sites.append(usefulness)\n",
    "    # print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we implement feature selection. For this kind of data, that is, documents composed of many words, using bag of words is a sensible approach.\n",
    "It vectorizes whole documents by storing the amount of times each unique word in the vocabulary has appeared on it, using a very efficient data structure for spending less memory (sparse matrices from scipy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 5460)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_sites = count_vect.fit_transform(sites_contents)\n",
    "X_sites.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's a good idea to normalize this vector. As it stands, it counts the absolute number of occurrences. But some documents may be longer than others and this fact can cause inaccuracies down the line. We will transform the occurrences count into a frequency measure (term frequency - tf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 5460)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# We downscale the weight of too frequent words by turning the use_idf to True.\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_sites)\n",
    "X_sites_tf = tf_transformer.transform(X_sites)\n",
    "X_sites_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_sites_tf, y_sites, random_state=1337)\n",
    "X_train = X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now call transform instead of fit_transform in count_vect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bow_list_from_urls(url_list: list):\n",
    "    \"\"\"\n",
    "    Function to get the content and process it properly for predictions.\n",
    "    If training is True, call fit_transform to generate\n",
    "    \"\"\"\n",
    "    contents_list = []\n",
    "\n",
    "    for url in url_list:\n",
    "        r = requests.get(url)\n",
    "        filtered_content = remove_tags(r.content)\n",
    "        contents_list.append(filtered_content)\n",
    "    \n",
    "    filtered_contents_bow = count_vect.transform(contents_list)\n",
    "    filtered_contents_bow_tf = tf_transformer.transform(filtered_contents_bow)\n",
    "    return filtered_contents_bow_tf\n",
    "\n",
    "bow_list = generate_bow_list_from_urls(testing_url_list)\n",
    "bow_list = bow_list.toarray()\n",
    "\n",
    "# Creating a holding dict for all models predictions.\n",
    "predictions = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to choose a model and fit it with the training set.\n",
    "There are many choices here. We will start with basic gaussian naive bayes, and change later to different models to see how they fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acertou\n",
      "errou\n",
      "acertou\n",
      "acertou\n",
      "errou\n",
      "acertou\n",
      "acertou\n",
      "acertou\n",
      "acertou\n",
      "acertou\n",
      "errou\n",
      "acertou\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "predictions['gaussian_naive_bayes'] = gnb.predict(bow_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now testing with Multinomial Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnnb = MultinomialNB()\n",
    "mnnb.fit(X_train, y_train)\n",
    "predictions['multinomial_naive_bayes'] = mnnb.predict(bow_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now testing with decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt =  DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "predictions['decision_tree'] = dt.predict(bow_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
