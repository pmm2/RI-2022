{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data_file = open(\"./data_sets.json\")\n",
    "raw_data = json.load(data_file)\n",
    "data_file.close()\n",
    "\n",
    "training_sites = raw_data.get(\"trainingSitesAndTags\", {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treating raw data for training models.\n",
    "First, we need to actually retrieve the content present in the url.\n",
    "Then, we strip the content of its HTML and style tags, as they are irrelevant to our classification. For this, we use BeautifulSoup decompose function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to remove tags\n",
    "def remove_tags(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "  \n",
    "    for data in soup(['style', 'script']):\n",
    "        data.decompose()\n",
    "  \n",
    "    return ' '.join(soup.stripped_strings)\n",
    "\n",
    "sites_contents = []\n",
    "y_sites = []\n",
    "\n",
    "for [url, usefulness] in training_sites:\n",
    "    r = requests.get(url)\n",
    "    filtered_content = remove_tags(r.content)\n",
    "    sites_contents.append(filtered_content)\n",
    "    y_sites.append(usefulness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we implement feature selection. For this kind of data, that is, documents composed of many words, using bag of words is a sensible approach.\n",
    "It vectorizes whole documents by storing the amount of times each unique word in the vocabulary has appeared on it, using a very efficient data structure for spending less memory (sparse matrices from scipy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 4190)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_sites = count_vect.fit_transform(sites_contents)\n",
    "X_sites.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's a good idea to normalize this vector. As it stands, it counts the absolute number of occurrences. But some documents may be longer than others and this fact can cause inaccuracies down the line. We will transform the occurrences count into a frequency measure (term frequency - tf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 4190)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# We downscale the weight of too frequent words by turning the use_idf to True.\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_sites)\n",
    "X_sites_tf = tf_transformer.transform(X_sites)\n",
    "X_sites_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_sites_tf, y_sites, random_state=1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to choose a model and fit it with the training set.\n",
    "There are many choices here. We will start with basic gaussian naive bayes, and change later to a specific variant, the multinomial naive bayes, that is said to be the most suitable for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_list(l):\n",
    "    \"\"\" Function to unzip list \"\"\"\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    for [item1, item2] in l:\n",
    "        l1.append(item1)\n",
    "        l2.append(item2)\n",
    "    return l1, l2\n",
    "\n",
    "testing_sites = raw_data.get(\"testingSitesAndTags\", {})\n",
    "url_list, tags_list = unzip_list(testing_sites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now call transform instead of fit_transform in count_vect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bow_list_from_urls(url_list: list, training: bool = True):\n",
    "    \"\"\"\n",
    "    Function to get the content and process it properly for predictions.\n",
    "    If training is True, call fit_transform to generate\n",
    "    \"\"\"\n",
    "    contents_list = []\n",
    "\n",
    "    for url in url_list:\n",
    "        r = requests.get(url)\n",
    "        filtered_content = remove_tags(r.content)\n",
    "        contents_list.append(filtered_content)\n",
    "    \n",
    "    if training:\n",
    "        filtered_contents_bow = count_vect.fit_transform(contents_list)\n",
    "    else:\n",
    "        filtered_contents_bow = count_vect.transform(contents_list)\n",
    "    filtered_contents_bow_tf = tf_transformer.transform(filtered_contents_bow)\n",
    "    return filtered_contents_bow_tf\n",
    "\n",
    "bow_list = generate_bow_list_from_urls(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "errou\n",
      "acertou\n",
      "acertou\n",
      "acertou\n",
      "acertou\n",
      "errou\n",
      "acertou\n",
      "errou\n",
      "acertou\n",
      "acertou\n",
      "acertou\n",
      "acertou\n"
     ]
    }
   ],
   "source": [
    "def classify_usefulness(model, bow_list) -> list:\n",
    "    \"\"\"\n",
    "    Function to predict the usefulness of the contents of a url,\n",
    "    using the given model.\n",
    "    \"\"\"\n",
    "    predictions_list = model.predict(bow_list.toarray())\n",
    "    return predictions_list\n",
    "\n",
    "\n",
    "predictions = classify_usefulness(gnb, bow_list)\n",
    "for i in range(len(tags_list)):\n",
    "    if predictions[i] == tags_list[i]:\n",
    "        print(\"acertou\")\n",
    "    else:\n",
    "        print(\"errou\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
