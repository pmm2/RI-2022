{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processamento de Consulta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "limpando o input da consulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unidecode\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def clear_query_input(query, field=None, usestopwords=False):\n",
    "\n",
    "    # separa as palavras da query\n",
    "    if type(query) == str:\n",
    "        substr = re.findall(r'\"(.*?)\"', query)\n",
    "        words = re.sub(r'\"(.*?)\"', '', query)\n",
    "        words = words.lower().strip().split(' ')\n",
    "        words = words + substr\n",
    "   \n",
    "    # remove simbolos da query\n",
    "    toRemove = r'[.*,;\\(\\)\\'\\\"\\?\\!%\\$]'\n",
    "    for i in range(len(words)):\n",
    "        words[i] = re.sub(toRemove, '', words[i])\n",
    "        words[i] = unidecode.unidecode(words[i])\n",
    "    \n",
    "    # coloca os stop words em caso decidirmos usa-los \n",
    "    if usestopwords:\n",
    "        set_sw = set(stopwords.words('english'))\n",
    "        newWords = []\n",
    "        for word in words:\n",
    "            if not word in set_sw:\n",
    "                if (field):\n",
    "                    newWords.append(word + '-' + field)\n",
    "                else:\n",
    "                    newWords.append(word)\n",
    "        return list(set(newWords))\n",
    "    \n",
    "    # caso ele seja a consulta seja de um campo especifico, ele Ã© atribuido as palavras\n",
    "    if (field):\n",
    "        for i in range(len(words)):\n",
    "            words[i] = words[i] + '-' + field\n",
    "    return list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pequena', 'uma', 'consulta']\n",
      "['uma-title', 'pequena-title', 'consulta-title']\n"
     ]
    }
   ],
   "source": [
    "exemplo = 'Uma pequena consulta'\n",
    "print(clear_query_input(exemplo))\n",
    "print(clear_query_input(exemplo, field='title'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranqueamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "usando os pesos com tfidf: tf * idf\n",
    "    para o termo-documento = f(i,j) * log(N/ni)\n",
    "    para a termo-consulta = (0.5 * 0.5(f(i,q)/max(i)f(i,q)) * log(N/ni)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import log\n",
    "\n",
    "def get_idf(posting, num_docs):\n",
    "\n",
    "    idf = log(num_docs/len(posting))\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_Score(terms, tfidf, invlist, num_docs, doc_lengths, n):\n",
    "    scores = {i: 0 for i in range(1, num_docs + 1)}\n",
    "    # term-at-a-time\n",
    "    for term in terms:\n",
    "\n",
    "        w_term_query = terms.count(term)\n",
    "        if (tfidf):\n",
    "            w_term_query = 0.5 + 0.5 * \\\n",
    "                (w_term_query /\n",
    "                 (max([i for i, val in enumerate(terms) if val == term])+1))\n",
    "            w_term_query *= get_idf(invlist[term], num_docs) \n",
    "        for post in invlist[term]:\n",
    "            # print(post)\n",
    "            w_doc_term = post[1]  # tf\n",
    "            if tfidf:\n",
    "                w_doc_term *= get_idf(invlist[term], num_docs)\n",
    "            scores[post[0]] += w_term_query * w_doc_term\n",
    "    for d in range(num_docs):\n",
    "        # print(doc_lengths)\n",
    "        scores[d+1] /= doc_lengths[f'{d+1}']\n",
    "    keys_scores = sorted(scores, key=scores.get, reverse=True)\n",
    "    # print(scores)\n",
    "    return {keys_scores[i]: scores[keys_scores[i]] for i in range(5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_docs_len(docs_path_map='./data/data_map.json'):\n",
    "    lengths = {}\n",
    "    with open(docs_path_map, 'r') as map:\n",
    "        map_docs = map.read()\n",
    "        map_docs = json.loads(map_docs)\n",
    "    for id in map_docs:\n",
    "        with open(f\"../data/htmls/{map_docs[id]['arquivo']}.html\") as html:\n",
    "            lengths[id] = len(BeautifulSoup(\n",
    "                html.read(), 'html.parser').text.split(' '))\n",
    "    return lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pegando tamanho dos arquivos\n",
    "docs_len = get_docs_len(docs_path_map='../data/data_map.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_invindex(path='../inverted_index/geral_invindex.json'):\n",
    "    with open(path, 'r') as file:\n",
    "        text = file.read()\n",
    "        invindex = json.loads(text)\n",
    "    return invindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query, tfidf, num_docs, docs_lenght, n, field=None, invlist_path='../inverted_index/geral_invindex.json'):\n",
    "    terms = clear_query_input(query, field)\n",
    "    invindex = get_invindex(invlist_path)\n",
    "    return cosine_Score(terms, tfidf, invindex, num_docs, docs_lenght, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_query(query_string, title, gen, dev, plat, price, num_docs, docs_lenght, tfidf=True, n=5):\n",
    "\n",
    "    games_list = []\n",
    "    if query_string != '':\n",
    "\n",
    "        games = run_query(query_string, tfidf, num_docs, docs_lenght, n)\n",
    "        games_list.append(games)\n",
    "\n",
    "    if title != '':\n",
    "\n",
    "        games = run_query(title, tfidf, num_docs,\n",
    "                          docs_lenght, n, field='title', invlist_path='../inverted_index/fields_invindex.json')\n",
    "        games_list.append(games)\n",
    "\n",
    "    if gen != '':\n",
    "\n",
    "        games = run_query(\n",
    "            gen, tfidf, num_docs, docs_lenght, n, field='genre', invlist_path='../inverted_index/fields_invindex.json')\n",
    "        games_list.append(games)\n",
    "\n",
    "    if dev != '':\n",
    "\n",
    "        games = run_query(\n",
    "            dev, tfidf, num_docs, docs_lenght, n, field='dev', invlist_path='../inverted_index/fields_invindex.json')\n",
    "        games_list.append(games)\n",
    "\n",
    "    if plat != '':\n",
    "\n",
    "        games = run_query(\n",
    "            plat, tfidf, num_docs, docs_lenght, n, field='plataforma', invlist_path='../inverted_index/fields_invindex.json')\n",
    "        games_list.append(games)\n",
    "\n",
    "    if price != '':\n",
    "\n",
    "        games = run_query(\n",
    "            price, tfidf, num_docs, docs_lenght, n, field='price', invlist_path='../inverted_index/fields_invindex.json')\n",
    "        games_list.append(games)\n",
    "\n",
    "    # print(games_list)\n",
    "    all_games = {}\n",
    "    for ranks_list in games_list:\n",
    "        for game in ranks_list:\n",
    "            if not str(game) in all_games:\n",
    "                all_games[str(game)] = float(ranks_list[game])\n",
    "            else:\n",
    "                all_games[str(game)] += float(ranks_list[game])\n",
    "    for doc in all_games:\n",
    "        all_games[doc] /= 6\n",
    "    print(all_games)\n",
    "    return sorted(all_games)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_map = {}\n",
    "with open('../data/data_map.json') as infile:\n",
    "    text = infile.read()\n",
    "    data_map = json.loads(text)\n",
    "qt_docs = len(data_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'3': 0.00022263975667812265, '37': 0.0002224151402972932, '281': 0.00021516483219078373, '276': 0.00021089721338904052, '263': 0.00021028031959431228}\n",
      "['263', '276', '281', '3', '37']\n",
      "{'73': 1.2361432721097838e-05, '1': 1.2351395016898815e-05, '69': 1.2350559277334778e-05, '52': 1.2333868204590222e-05, '55': 1.2322211271162538e-05, '132': 0.00017867587854341054, '38': 0.00017851915589169577, '37': 0.0001779548298002975, '76': 0.00017794286162289483, '39': 0.00017789500500711513}\n",
      "['1', '132', '37', '38', '39', '52', '55', '69', '73', '76']\n"
     ]
    }
   ],
   "source": [
    "print(execute_query(query_string='mario', title='', gen='', dev='', plat='', price='', num_docs=qt_docs, docs_lenght=docs_len))\n",
    "\n",
    "print(execute_query(query_string='', title='ps4', gen='esportes', dev='', plat='', price='', num_docs=qt_docs, docs_lenght=docs_len))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
