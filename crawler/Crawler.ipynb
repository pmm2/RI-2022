{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crawler.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " Lista de sites (roots):\n",
        "\n",
        "* https://www.amazon.com.br/\n",
        "* https://www.americanas.com.br/ <- resolver timeout\n",
        "* https://www.bigboygames.com.br/\n",
        "* https://www.mercadolivre.com.br/\n",
        "* https://www.magazineluiza.com.br/\n",
        "* https://www.submarino.com.br/\n",
        "* https://www.buscape.com.br/\n",
        "* https://www.maurospbrgames.com.br/ <- pode ser tirado dependendo da situação\n",
        "* https://shopee.com.br/\n",
        "* https://jogosbarato.com/ <- possuie as informções mas sem ser em tabela\n",
        "* https://www.kabum.com.br/  <- possuie as informções mas sem ser em tabela\n",
        "* https://www.tcagames.com.br/  <- possuie as informções mas sem ser em tabela\n",
        "* https://lojaarenagames.com.br/ \n",
        "* https://www.shockgames.com.br/\n",
        "* https://www.baraogames.com/\n",
        "* https://www.xplacegames.com.br/\n",
        "* https://www.futuristicgames.com.br/\n",
        "* https://www.vnsgames.com.br/\n",
        "* https://www.pontofrio.com.br/\n",
        "* https://www.ibyte.com.br/\n",
        "* https://www.extra.com.br/\n",
        "* https://www.fastshop.com.br/"
      ],
      "metadata": {
        "id": "AyicXZdyCsgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install validators"
      ],
      "metadata": {
        "id": "-EgwrN4yy4G-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b5cc7ee-e07d-4cbf-d589-130b120607c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting validators\n",
            "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators) (4.4.2)\n",
            "Building wheels for collected packages: validators\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19582 sha256=1599759e49721a0c187584c446308abedb2a606e4798ceb35ddf1232b14e6c08\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/55/ab/36a76989f7f88d9ca7b1f68da6d94252bb6a8d6ad4f18e04e9\n",
            "Successfully built validators\n",
            "Installing collected packages: validators\n",
            "Successfully installed validators-0.20.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen, Request\n",
        "from threading import Thread\n",
        "import urllib.robotparser as urobot\n",
        "import urllib.error\n",
        "import re\n",
        "import socket\n",
        "import validators\n",
        "import time\n",
        "\n",
        "\n",
        "sites = [\n",
        "            \"https://www.bigboygames.com.br/\",\n",
        "            \"https://www.magazineluiza.com.br/\",\n",
        "            # \"https://www.kabum.com.br/\", --> resolver conecção\n",
        "            # \"https://lojaarenagames.com.br/\", --> parser robot.txt com negando todos os links\n",
        "            \"https://www.shockgames.com.br/\",\n",
        "            # \"https://www.futuristicgames.com.br/\", --> parser robot.txt dando problema\n",
        "            \"https://www.vnsgames.com.br/\",\n",
        "            \"https://www.ibyte.com.br/\",\n",
        "            # \"https://www.extra.com.br/\", -->resolver timeout\n",
        "            # \"https://www.fastshop.com.br/\" --> parser robot.txt com negando todos os links\n",
        "        ]\n",
        "\n",
        "delay = 1 #em segundos\n",
        "qtpaginas = 5\n",
        "\n",
        "\n",
        "\n",
        "#fronteira\n",
        "class Frontier:\n",
        "    def __init__(self, root):\n",
        "        self.list = [root]\n",
        "        self.looked = set()\n",
        "\n",
        "    def get_next_url(self):\n",
        "        nxt = self.list.pop(0)\n",
        "        while nxt in self.looked:\n",
        "            nxt = self.list.pop(0)\n",
        "        self.looked.add(nxt)\n",
        "        return nxt\n",
        "\n",
        "    def add_urls(self, urls):\n",
        "        self.list += urls\n",
        "\n",
        "#pega a pagina html\n",
        "def get_html_page(url):\n",
        "    if validators.url(url):\n",
        "        try:\n",
        "            req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "            page = urlopen(req, timeout=5)\n",
        "        except urllib.error.HTTPError as err:\n",
        "            # save the url an the erro but still running\n",
        "            print('http-error')\n",
        "            return None\n",
        "        except urllib.error.URLError as err:\n",
        "            print(\"url-error\")\n",
        "            # save the url an the erro but still running\n",
        "            return None\n",
        "        except socket.timeout:\n",
        "            print(\"socket timeout\")\n",
        "        else:\n",
        "            if page.headers.get_content_type() ==\"text/html\":\n",
        "                return BeautifulSoup(page, \"html.parser\")\n",
        "            else:\n",
        "                return None\n",
        "    else: \n",
        "        return None\n",
        "\n",
        "\n",
        "#extrai os links da pagina para a fronteira\n",
        "def extract_links(page, root, robot):\n",
        "    robot.read()\n",
        "    links = []\n",
        "    for link in page.findAll('a', href=True):\n",
        "        href = link.get('href')\n",
        "        if not re.search(\"^https:\", href):\n",
        "            href = root + href\n",
        "        if re.search(rf\"^{root}\", href):\n",
        "            if robot.can_fetch(\"*\", href):\n",
        "                links.append(href)\n",
        "    return links\n",
        "\n",
        "\n",
        "#salva a pagina html\n",
        "\n",
        "def save_page(page, root, cont):\n",
        "      print(root + \": \"+ cont)\n",
        "#     soup = str(page)\n",
        "#     site_name = root[8:len(root)-1]\n",
        "#     Path('./' + site_name).mkdir(parents=True, exist_ok=True)\n",
        "#     html_file = open(site_name + \"/\" + cont + \".html\", \"w\")\n",
        "#     html_file.write(soup)\n",
        "#     html_file.close()\n",
        "#     print(\".\")\n",
        "\n",
        "\n",
        "\n",
        "#bot do crawler\n",
        "class Bot:\n",
        "    def __init__(self, root):\n",
        "        self.root = root\n",
        "        self.frontier = Frontier(root)\n",
        "        self.robot = urobot.RobotFileParser()\n",
        "        self.robot.set_url(root + 'robots.txt')\n",
        "\n",
        "    def run(self, delay, n_pages):\n",
        "        count = 1\n",
        "        while n_pages > 0:\n",
        "\n",
        "            url = self.frontier.get_next_url()\n",
        "            page = get_html_page(url)\n",
        "            if page is not None:\n",
        "                urls = extract_links(page, self.root, self.robot)\n",
        "                self.frontier.add_urls(urls)\n",
        "                save_page(page, url, str(count))\n",
        "                count += 1\n",
        "                n_pages -= 1\n",
        "            time.sleep(delay)\n",
        "\n",
        "\n",
        "def main():\n",
        "    bot = None\n",
        "    for root in sites:\n",
        "        bot = Bot(root)\n",
        "        bot.run(delay,qtpaginas)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "jlIke6yk2w8l",
        "outputId": "62aa3c00-2b95-446a-c157-30b1efb1dfec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.fastshop.com.br/: 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-8a17888ff828>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-8a17888ff828>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msites\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mbot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mbot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mqtpaginas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-8a17888ff828>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, delay, n_pages)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mn_pages\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrontier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_html_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-8a17888ff828>\u001b[0m in \u001b[0;36mget_next_url\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mnxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mnxt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlooked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mnxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: pop from empty list"
          ]
        }
      ]
    }
  ]
}