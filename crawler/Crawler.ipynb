{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crawler.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **RI - Recuperação de Informação - Projeto 1**\n",
        "\n",
        "# grupo:\n",
        "\n",
        "\n",
        "*   André Luis - alps2\n",
        "*   João Plácido - jpsn\n",
        "*   Pedro Milet - pmm2\n",
        "\n",
        "# Domínio/Tópico: Jogos de midia fisica\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VvIRKf9VwKeb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Crawler**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f9qIYvx1TCXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Instalando dependencias**"
      ],
      "metadata": {
        "id": "uaKf0e4q_pc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install validators"
      ],
      "metadata": {
        "id": "-EgwrN4yy4G-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11f66c25-7fa8-4312-e763-ae26f75ffb38"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.7/dist-packages (0.20.0)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators) (4.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **iniciar variaveis e imports**"
      ],
      "metadata": {
        "id": "gsoHA4R2IZdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen, Request\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.robotparser as urobot\n",
        "import concurrent.futures\n",
        "import urllib.error\n",
        "import validators\n",
        "import socket\n",
        "import time\n",
        "import re\n",
        "#lista de dominios a serem coletados\n",
        "urls = [\n",
        "            \"https://www.bigboygames.com.br/\",\n",
        "            \"https://www.mercadolivre.com.br/\",\n",
        "            \"https://www.shopb.com.br/\",\n",
        "            \"https://www.shockgames.com.br/\",\n",
        "            \"https://www.vnsgames.com.br/\",\n",
        "            \"https://www.ibyte.com.br/\"\n",
        "    ]\n",
        "\n",
        "delay = 1 # tempo em segundos parar fazer uma requisição\n",
        "\n",
        "qt_paginas = 10 #quantidades de paginas a serem coletadas por dominio "
      ],
      "metadata": {
        "id": "MjWmzWmrIl9L"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fronteira**"
      ],
      "metadata": {
        "id": "hhEBv0BM_cS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fronteira busca em largura\n",
        "class Frontier:\n",
        "    def __init__(self, root):\n",
        "        self.list = [root]\n",
        "        self.looked = set()\n",
        "\n",
        "    def get_next_url(self):\n",
        "        nxt = self.list.pop(0)\n",
        "        self.looked.add(nxt)\n",
        "        return nxt\n",
        "\n",
        "    def add_urls(self, urls):\n",
        "      for url in urls:\n",
        "        if url not in self.looked:\n",
        "          self.list.append(url)\n",
        "\n",
        "\n",
        "#fronteira com euristica Inlink\n",
        "class FrontierInlink:\n",
        "\n",
        "    def __init__(self, root):\n",
        "        self.list = dict()\n",
        "        self.list[root] = 0\n",
        "        self.looked = set()\n",
        "\n",
        "    def get_next_url(self):\n",
        "        max_inlink_url = max(self.list,key=self.list.get)\n",
        "        del self.list[max_inlink_url]\n",
        "        self.looked.add(max_inlink_url)\n",
        "        return max_inlink_url\n",
        "\n",
        "    def add_urls(self, urls):\n",
        "        for url in urls:\n",
        "\n",
        "            if url not in self.looked:\n",
        "\n",
        "                if url not in self.list:\n",
        "                    self.list[url] = 0\n",
        "\n",
        "            self.list[url] += 1\n"
      ],
      "metadata": {
        "id": "8-Pfiklm_29a"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Metodos auxiliares**"
      ],
      "metadata": {
        "id": "iN7YQzRYAjPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen, Request\n",
        "import urllib.error\n",
        "import validators\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "# header com user agent para fazer as requisições\n",
        "header = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36'}\n",
        "\n",
        "#função para pegar o html\n",
        "def get_html_page(url, delay):\n",
        "    \n",
        "    # valida se a string está em um formato de url\n",
        "    if validators.url(url):\n",
        "\n",
        "        try:\n",
        "            time.sleep(delay)\n",
        "            req = Request(url,headers=header)\n",
        "            page = urlopen(req, timeout=5)\n",
        "\n",
        "        # tramento dos erros\n",
        "        except urllib.error.HTTPError as err:\n",
        "            # save the http an the erro but still running\n",
        "            print('\\n' + url + ': ' + str(err.getcode()))\n",
        "            return None\n",
        "        except urllib.error.URLError as err:\n",
        "            # save the url an the erro but still running\n",
        "            print('\\n' + url + ': ' + str(err.args))\n",
        "            return None\n",
        "        except socket.timeout:\n",
        "            print(\"socket timeout\")\n",
        "\n",
        "        else:\n",
        "            #checa se é um html\n",
        "            if page.headers.get_content_type() == \"text/html\":\n",
        "                return BeautifulSoup(page, \"html.parser\")\n",
        "            else:\n",
        "                return None\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "#extrai os links da pagina html\n",
        "def extract_links(page, root, robot):\n",
        "    robot.read()\n",
        "    links = []\n",
        "\n",
        "    for link in page.findAll('a', href=True):\n",
        "        href = link.get('href')\n",
        "        # print(href)\n",
        "        if not re.search(\"^https:\", href):\n",
        "            href = root + href\n",
        "        if re.search(rf\"^{root}\", href):\n",
        "            if robot.can_fetch(\"*\", href):\n",
        "                links.append(href)\n",
        "    return links\n",
        "\n",
        "#salva o html\n",
        "def save_html(page, root, count):\n",
        "    \n",
        "    html = str(page)\n",
        "    file = open(root[8:len(root)-1] + \"/\" + str(count) + '.html',\"w\")\n",
        "    file.write(html)\n",
        "    file.close\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C2twLcnzBP4y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Crawler bot**"
      ],
      "metadata": {
        "id": "K7yVIpqXFfr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Bot:\n",
        "    def __init__(self, root, euristica):\n",
        "        self.root = root\n",
        "\n",
        "        #iniciando a fronteira de de acordo com euristica\n",
        "        if euristica:\n",
        "            self.frontier = FrontierInlink(root)\n",
        "        else:\n",
        "            self.frontier = Frontier(root)\n",
        "        \n",
        "        #definindo um parser para o robot.txt\n",
        "        self.robot = urobot.RobotFileParser()\n",
        "        self.robot.set_url(root + 'robots.txt')\n",
        "\n",
        "    def run(self, delay, n_pages):\n",
        "        \n",
        "        qt_pages = n_pages\n",
        "        count = 1\n",
        "\n",
        "        #roda até coletar uma quantidade de paginas ou a fronteira acabar\n",
        "        while qt_pages > 0 and len(self.frontier.list) > 0:\n",
        "            \n",
        "            #pega proximo da fronteira\n",
        "            url = self.frontier.get_next_url()\n",
        "            \n",
        "            #tenta coletar o html \n",
        "            page = get_html_page(url, delay)\n",
        "            \n",
        "            #se conseguir coletar (se não deu erro)\n",
        "            if page is not None:\n",
        "\n",
        "                #extrai \n",
        "                urls = extract_links(page, self.root, self.robot)\n",
        "                self.frontier.add_urls(urls)\n",
        "                save_html(page, self.root, count)\n",
        "                count += 1\n",
        "                qt_pages -= 1\n",
        "\n",
        "        # print(self.root +' ok')\n",
        "        return self.root + ': terminado'"
      ],
      "metadata": {
        "id": "tzOhOp9lFfXV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main**"
      ],
      "metadata": {
        "id": "UXoTKiO9IL8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  bots = []\n",
        "  for url in urls:\n",
        "    bots.append(Bot(url, False))\n",
        "\n",
        "  with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    futures = []\n",
        "    for bot in bots:\n",
        "      futures.append(executor.submit(bot.run, delay=delay, n_pages=qt_paginas, ))\n",
        "    for future in concurrent.futures.as_completed(futures):\n",
        "      print(future.result())\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o54YeIH6IOkK",
        "outputId": "98bbe41f-0ae2-4686-c87c-8dfc50e77ed2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "https://www.mercadolivre.com.br///www.mercadolivre.com.br: 404\n",
            "\n",
            "https://www.shockgames.com.br/javascript:;: 418\n",
            "\n",
            "https://www.shopb.com.br/javascript:;: 418\n",
            "\n",
            "https://www.shopb.com.br/javascript:;: 418\n",
            "https://www.shockgames.com.br/: terminado\n",
            "https://www.ibyte.com.br/: terminado\n",
            "https://www.shopb.com.br/: terminado\n",
            "https://www.vnsgames.com.br/: terminado\n",
            "https://www.mercadolivre.com.br/: terminado\n",
            "https://www.bigboygames.com.br/: terminado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "k_mMHCH8PI4Q"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}